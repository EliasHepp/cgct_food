{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sensitivity analysis - Baseline\n",
    "\n",
    "# Real-world - train\n",
    "df_all = pd.read_csv('data/food_df_rob.csv') \n",
    "year = 2018\n",
    "df = df_all[df_all['year'] == year]\n",
    "\n",
    "# Prepare real world data\n",
    "X = df[['ad_exc2_aid_gdp', 'infl', 'food_prod', 'food_imp_net_gdp',\n",
    "        'log_gdp', 'log_gdp_pc', 'gdp_pc_grw', 'gov_score', 'e_pt_coup', 'disast_affect', 'fight', 'prec_z', 'net_dens']].to_numpy()\n",
    "A = df['ad_sdg2_aid'].to_numpy()\n",
    "Y = df['change_msfi'].to_numpy()\n",
    "\n",
    "n = X.shape[0]\n",
    "p = X.shape[1]\n",
    "\n",
    "# Data standardization: min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data[:,2:])\n",
    "data_train = np.concatenate([data[:,0:2], data_scaled], axis=1)\n",
    "\n",
    "# Hyperpar list\n",
    "hyper_opt_list = open(\"hyperpars/hyperpars_opt_real.txt\", \"r\")\n",
    "hyper_opt_list = hyper_opt_list.read()\n",
    "hyper_opt = ast.literal_eval(hyper_opt_list)\n",
    "opt_hyperpars = hyper_opt[-5]\n",
    "\n",
    "# Set all seeds\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Estimate model\n",
    "data_train_bae, mod_BAE = BAE(data_train, opt_hyperpars)    \n",
    "device = torch.device('cpu')        \n",
    "aid_max = np.max(data_train[:,1])\n",
    "aid_min = np.min(data_train[:,1])\n",
    "aid_random = np.random.uniform(aid_min, aid_max, (data_train_bae.shape[0], opt_hyperpars['m_scw']))\n",
    "data_train_scw = SCw(data_train_bae, aid_random, opt_hyperpars)\n",
    "_, _, a_base = GPS(data_train_scw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sensitivity analysis w.r.t. past aid of neighbor countries\n",
    "\n",
    "# Real-world - train\n",
    "df_all = pd.read_csv('data/food_df_rob.csv') \n",
    "year = 2018\n",
    "df = df_all[df_all['year'] == year]\n",
    "\n",
    "# Prepare real world data\n",
    "X = df[['ad_exc2_aid_gdp', 'infl', 'food_prod', 'food_imp_net_gdp',\n",
    "        'log_gdp', 'log_gdp_pc', 'gdp_pc_grw', 'gov_score', 'e_pt_coup', 'disast_affect', 'fight', 'prec_z', 'net_dens']].to_numpy()\n",
    "A = df['ad_sdg2_aid'].to_numpy()\n",
    "Y = df['change_msfi'].to_numpy()\n",
    "\n",
    "n = X.shape[0]\n",
    "p = X.shape[1] + 1\n",
    "\n",
    "# Add covariate\n",
    "borders = pd.read_csv('data/borders.csv')\n",
    "borders = borders[borders['country_code'].isin(df['country'])]\n",
    "borders = borders[borders['country_border_code'].isin(df['country'])]\n",
    "past_aid_neigh = np.empty((n,1))\n",
    "for i in range(n):\n",
    "    ctry = df['country'].values[i]\n",
    "    neighs = borders[borders['country_code'] == ctry]['country_border_code'].values\n",
    "    if len(neighs) > 0:\n",
    "        past_aid_neigh[i] = np.mean(df_all[(df_all['year'] == 2017) & (df_all['country'].isin(neighs))]['ad_sdg2_aid'])\n",
    "    else:\n",
    "        past_aid_neigh[i] = df_all[(df_all['year'] == 2017) & (df_all['country'] == ctry)]['ad_sdg2_aid']\n",
    "    \n",
    "data = np.concatenate([Y.reshape(n,1), A.reshape(n,1), X, past_aid_neigh],axis=1)\n",
    "\n",
    "# Data standardization: min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data[:,2:])\n",
    "data_train = np.concatenate([data[:,0:2], data_scaled], axis=1)\n",
    "\n",
    "# Hyperpar list\n",
    "hyper_opt_list = open(\"hyperpars/hyperpars_opt_real.txt\", \"r\")\n",
    "hyper_opt_list = hyper_opt_list.read()\n",
    "hyper_opt = ast.literal_eval(hyper_opt_list)\n",
    "opt_hyperpars = hyper_opt[-5]\n",
    "\n",
    "# Set all seeds\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Estimate model\n",
    "data_train_bae, mod_BAE = BAE(data_train, opt_hyperpars)    \n",
    "device = torch.device('cpu')        \n",
    "aid_max = np.max(data_train[:,1])\n",
    "aid_min = np.min(data_train[:,1])\n",
    "aid_random = np.random.uniform(aid_min, aid_max, (data_train_bae.shape[0], opt_hyperpars['m_scw']))\n",
    "data_train_scw = SCw(data_train_bae, aid_random, opt_hyperpars)\n",
    "_, _, a_neighaid = GPS(data_train_scw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sensitivity analysis w.r.t. past aid \n",
    "\n",
    "# Real-world - train\n",
    "df_all = pd.read_csv('data/food_df_ana.csv') \n",
    "year = 2018\n",
    "df = df_all[df_all['year'] == year]\n",
    "\n",
    "# Prepare real world data\n",
    "X = df.iloc[:, 5:].to_numpy()\n",
    "A = df.iloc[:, 4].to_numpy()\n",
    "Y = df.iloc[:, 2].to_numpy()\n",
    "\n",
    "n = X.shape[0]\n",
    "p = X.shape[1] + 1\n",
    "\n",
    "# Add covariate\n",
    "past_aid = df_all[df_all['year'] == 2017]['ad_sdg2_aid'].values.reshape(n,1)\n",
    "\n",
    "data = np.concatenate([Y.reshape(n,1), A.reshape(n,1), X, past_aid],axis=1)\n",
    "\n",
    "# Data standardization: min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data[:,2:])\n",
    "data_train = np.concatenate([data[:,0:2], data_scaled], axis=1)\n",
    "\n",
    "# Hyperpar list\n",
    "hyper_opt_list = open(\"hyperpars/hyperpars_opt_real.txt\", \"r\")\n",
    "hyper_opt_list = hyper_opt_list.read()\n",
    "hyper_opt = ast.literal_eval(hyper_opt_list)\n",
    "opt_hyperpars = hyper_opt[-5]\n",
    "\n",
    "# Set all seeds\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Estimate model\n",
    "data_train_bae, mod_BAE = BAE(data_train, opt_hyperpars)    \n",
    "device = torch.device('cpu')        \n",
    "aid_max = np.max(data_train[:,1])\n",
    "aid_min = np.min(data_train[:,1])\n",
    "aid_random = np.random.uniform(aid_min, aid_max, (data_train_bae.shape[0], opt_hyperpars['m_scw']))\n",
    "data_train_scw = SCw(data_train_bae, aid_random, opt_hyperpars)\n",
    "_, _, a_pastaid = GPS(data_train_scw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sensitivity analysis w.r.t. past Insecurity Prevalence\n",
    "\n",
    "# Real-world - train\n",
    "df_all = pd.read_csv('data/food_df_rob.csv') \n",
    "year = 2018\n",
    "df = df_all[df_all['year'] == year]\n",
    "\n",
    "# Prepare real world data\n",
    "X = df[['ad_exc2_aid_gdp', 'infl', 'food_prod', 'food_imp_net_gdp',\n",
    "        'log_gdp', 'log_gdp_pc', 'gdp_pc_grw', 'gov_score', 'e_pt_coup', 'disast_affect', 'fight', 'prec_z', 'net_dens']].to_numpy()\n",
    "A = df['ad_sdg2_aid'].to_numpy()\n",
    "Y = df['change_msfi'].to_numpy()\n",
    "\n",
    "n = X.shape[0]\n",
    "p = X.shape[1] + 1\n",
    "\n",
    "# Add covariate\n",
    "past_prev_msfi = df_all[df_all['year'] == 2017]['prev_msfi'].values.reshape(n,1)\n",
    "\n",
    "data = np.concatenate([Y.reshape(n,1), A.reshape(n,1), X, past_prev_msfi],axis=1)\n",
    "\n",
    "# Data standardization: min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data[:,2:])\n",
    "data_train = np.concatenate([data[:,0:2], data_scaled], axis=1)\n",
    "\n",
    "# Hyperpar list\n",
    "hyper_opt_list = open(\"hyperpars/hyperpars_opt_real.txt\", \"r\")\n",
    "hyper_opt_list = hyper_opt_list.read()\n",
    "hyper_opt = ast.literal_eval(hyper_opt_list)\n",
    "opt_hyperpars = hyper_opt[-5]\n",
    "\n",
    "# Set all seeds\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Estimate model\n",
    "data_train_bae, mod_BAE = BAE(data_train, opt_hyperpars)    \n",
    "device = torch.device('cpu')        \n",
    "aid_max = np.max(data_train[:,1])\n",
    "aid_min = np.min(data_train[:,1])\n",
    "aid_random = np.random.uniform(aid_min, aid_max, (data_train_bae.shape[0], opt_hyperpars['m_scw']))\n",
    "data_train_scw = SCw(data_train_bae, aid_random, opt_hyperpars)\n",
    "_, _, a_pastins = GPS(data_train_scw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sensitivity analysis w.r.t. past Insecurity Prevalence of neighbor countries \n",
    "\n",
    "# Real-world - train\n",
    "df_all = pd.read_csv('data/food_df_rob.csv') \n",
    "year = 2018\n",
    "df = df_all[df_all['year'] == year]\n",
    "\n",
    "# Prepare real world data\n",
    "X = df[['ad_exc2_aid_gdp', 'infl', 'food_prod', 'food_imp_net_gdp',\n",
    "        'log_gdp', 'log_gdp_pc', 'gdp_pc_grw', 'gov_score', 'e_pt_coup', 'disast_affect', 'fight', 'prec_z', 'net_dens']].to_numpy()\n",
    "A = df['ad_sdg2_aid'].to_numpy()\n",
    "Y = df['change_msfi'].to_numpy()\n",
    "n = X.shape[0]\n",
    "p = X.shape[1] + 1\n",
    "\n",
    "# Add covariate\n",
    "borders = pd.read_csv('data/borders.csv')\n",
    "borders = borders[borders['country_code'].isin(df['country'])]\n",
    "borders = borders[borders['country_border_code'].isin(df['country'])]\n",
    "past_msfe_neigh = np.empty((n,1))\n",
    "for i in range(n):\n",
    "    ctry = df['country'].values[i]\n",
    "    neighs = borders[borders['country_code'] == ctry]['country_border_code'].values\n",
    "    if len(neighs) > 0:\n",
    "        past_hiv_neigh[i] = np.mean(df_all[(df_all['year'] == 2017) & (df_all['country'].isin(neighs))]['prev_msfi'])\n",
    "    else:\n",
    "        past_hiv_neigh[i] = df_all[(df_all['year'] == 2017) & (df_all['country'] == ctry)]['prev_msfi']\n",
    "    \n",
    "data = np.concatenate([Y.reshape(n,1), A.reshape(n,1), X, past_msfe_neigh],axis=1)\n",
    "\n",
    "# Data standardization: min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data[:,2:])\n",
    "data_train = np.concatenate([data[:,0:2], data_scaled], axis=1)\n",
    "\n",
    "# Hyperpar list\n",
    "hyper_opt_list = open(\"hyperpars/hyperpars_opt_real.txt\", \"r\")\n",
    "hyper_opt_list = hyper_opt_list.read()\n",
    "hyper_opt = ast.literal_eval(hyper_opt_list)\n",
    "opt_hyperpars = hyper_opt[-1]\n",
    "\n",
    "# Set all seeds\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Estimate model\n",
    "data_train_bae, mod_BAE = BAE(data_train, opt_hyperpars)    \n",
    "device = torch.device('cpu')        \n",
    "aid_max = np.max(data_train[:,1])\n",
    "aid_min = np.min(data_train[:,1])\n",
    "aid_random = np.random.uniform(aid_min, aid_max, (data_train_bae.shape[0], opt_hyperpars['m_scw']))\n",
    "data_train_scw = SCw(data_train_bae, aid_random, opt_hyperpars)\n",
    "_, _, a_pastinsneigh = GPS(data_train_scw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.03338767e-02],\n",
       "       [ 1.80656374e-04],\n",
       "       [-2.39857360e-07],\n",
       "       [ 6.68677886e+01],\n",
       "       [-1.35234097e+04],\n",
       "       [-9.69251322e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_base\n",
    "a_pastaid\n",
    "a_neighaid\n",
    "a_pastinsneigh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
